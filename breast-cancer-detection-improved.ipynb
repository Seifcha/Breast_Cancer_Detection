{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":408,"sourceType":"datasetVersion","datasetId":180}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Breast Cancer Detection: Data Preparation & Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 8)\n\nprint(\"✓ Libraries imported successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"1. Business Understandig","metadata":{}},{"cell_type":"markdown","source":"2. Data Understandig","metadata":{}},{"cell_type":"code","source":"# Load the Wisconsin Diagnostic Breast Cancer dataset\ndf = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Total samples: {df.shape[0]}\")\nprint(f\"Total features: {df.shape[1]}\")\n\n# Display first few rows\nprint(\"\\nFirst 5 rows of the dataset:\")\nprint(df.head(10))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for columns with all NaN values\nprint(\"\\nColumns with all NaN values:\")\nnan_cols = df.columns[df.isna().all()].tolist()\nprint(nan_cols if nan_cols else \"None\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CHECK FOR MISSING VALUES\n\nmissing_values = df.isnull().sum()\nmissing_percentage = (missing_values / len(df)) * 100\n\nmissing_df = pd.DataFrame({\n    'Column': missing_values.index,\n    'Missing_Count': missing_values.values,\n    'Percentage': missing_percentage.values\n})\nmissing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n\nif len(missing_df) > 0:\n    print(\"\\nColumns with missing values:\")\n    print(missing_df)\nelse:\n    print(\"\\n✓ No missing values found in the dataset\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DATA INFORMATION\n\nprint(\"\\nData types:\")\nprint(df.dtypes.value_counts())\n\nprint(\"\\nDetailed information:\")\ndf.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TARGET VARIABLE ANALYSIS\n\n# Check diagnosis distribution\ndiagnosis_counts = df['diagnosis'].value_counts()\ndiagnosis_percentages = df['diagnosis'].value_counts(normalize=True) * 100\n\nprint(\"\\nDiagnosis Distribution:\")\nprint(f\"Benign (B): {diagnosis_counts['B']} samples ({diagnosis_percentages['B']:.2f}%)\")\nprint(f\"Malignant (M): {diagnosis_counts['M']} samples ({diagnosis_percentages['M']:.2f}%)\")\n\n# Encode target variable: M (Malignant) = 1, B (Benign) = 0\ndf['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\nprint(\"\\n✓ Target variable encoded: M=1 (Malignant), B=0 (Benign)\")\n\n# Visualize class distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Count plot\ndiagnosis_labels = ['Benign', 'Malignant']\ndiagnosis_values = [diagnosis_counts['B'], diagnosis_counts['M']]\naxes[0].bar(diagnosis_labels, diagnosis_values, color=['#2ecc71', '#e74c3c'])\naxes[0].set_ylabel('Count')\naxes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\nfor i, v in enumerate(diagnosis_values):\n    axes[0].text(i, v + 5, str(v), ha='center', va='bottom', fontweight='bold')\n\n# Pie chart\ncolors = ['#2ecc71', '#e74c3c']\naxes[1].pie(diagnosis_values, labels=diagnosis_labels, autopct='%1.1f%%', \n            startangle=90, colors=colors, textprops={'fontsize': 12, 'fontweight': 'bold'})\naxes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Visualization saved as 'class_distribution.png'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE CATEGORIZATION\n\n# Categorize features as in the paper\nmean_features = [col for col in df.columns if col.endswith('_mean')]\nse_features = [col for col in df.columns if col.endswith('_se')]\nworst_features = [col for col in df.columns if col.endswith('_worst')]\n\nprint(f\"\\nMean features (10): {len(mean_features)}\")\nprint(mean_features)\nprint(f\"\\nStandard Error features (10): {len(se_features)}\")\nprint(se_features)\nprint(f\"\\nWorst features (10): {len(worst_features)}\")\nprint(worst_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STATISTICAL SUMMARY\n\n# Separate features by diagnosis\nbenign = df[df['diagnosis'] == 0]\nmalignant = df[df['diagnosis'] == 1]\n\nprint(\"\\nOverall Statistics:\")\nprint(df.describe())\n\nprint(\"Statistics for Benign Cases:\")\nprint(benign.describe())\n\nprint(\"Statistics for Malignant Cases:\")\nprint(malignant.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n#  EXPLORATORY DATA ANALYSIS\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPLORATORY DATA ANALYSIS\")\nprint(\"=\"*80)\n\n# 1 Distribution of Mean Features\nprint(\"\\n[1/7] Creating distribution plots for mean features...\")\nfig, axes = plt.subplots(5, 2, figsize=(15, 18))\naxes = axes.ravel()\n\nfor idx, col in enumerate(mean_features):\n    axes[idx].hist(benign[col], bins=30, alpha=0.6, label='Benign', color='#2ecc71', edgecolor='black')\n    axes[idx].hist(malignant[col], bins=30, alpha=0.6, label='Malignant', color='#e74c3c', edgecolor='black')\n    axes[idx].set_xlabel(col, fontsize=10)\n    axes[idx].set_ylabel('Frequency', fontsize=10)\n    axes[idx].set_title(f'Distribution of {col}', fontsize=11, fontweight='bold')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('mean_features_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved as 'mean_features_distribution.png'\")\n\n# 2 Box plots for mean features\nprint(\"\\n[2/7] Creating box plots for mean features...\")\nfig, axes = plt.subplots(5, 2, figsize=(15, 18))\naxes = axes.ravel()\n\nfor idx, col in enumerate(mean_features):\n    df.boxplot(column=col, by='diagnosis', ax=axes[idx])\n    axes[idx].set_xlabel('Diagnosis (0=Benign, 1=Malignant)', fontsize=10)\n    axes[idx].set_ylabel(col, fontsize=10)\n    axes[idx].set_title(f'Box Plot: {col}', fontsize=11, fontweight='bold')\n    axes[idx].get_figure().suptitle('')\n\nplt.tight_layout()\nplt.savefig('mean_features_boxplot.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved as 'mean_features_boxplot.png'\")\n\n# 3 Correlation Matrix\nprint(\"\\n[3/7] Creating correlation matrix...\")\n# Select only numeric columns (excluding diagnosis for now)\nnumeric_features = df.drop('diagnosis', axis=1)\n\nplt.figure(figsize=(20, 16))\ncorrelation_matrix = numeric_features.corr()\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n            linewidths=0.5, cbar_kws={\"shrink\": 0.8})\nplt.title('Correlation Matrix - All Features', fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.savefig('correlation_matrix_full.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved as 'correlation_matrix_full.png'\")\n\n# 4 Correlation with diagnosis\nprint(\"\\n[4/7] Analyzing correlation with diagnosis...\")\ncorrelations_with_diagnosis = df.corr()['diagnosis'].drop('diagnosis').sort_values(ascending=False)\n\nplt.figure(figsize=(12, 10))\ncorrelations_with_diagnosis.plot(kind='barh', color='steelblue')\nplt.xlabel('Correlation with Diagnosis', fontsize=12)\nplt.ylabel('Features', fontsize=12)\nplt.title('Feature Correlation with Diagnosis\\n(Positive = Malignant, Negative = Benign)', \n          fontsize=14, fontweight='bold')\nplt.axvline(x=0, color='red', linestyle='--', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('correlation_with_diagnosis.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved as 'correlation_with_diagnosis.png'\")\n\nprint(\"\\nTop 10 features most correlated with Malignant diagnosis:\")\nprint(correlations_with_diagnosis.head(10))\n\n# 9.5 Scatter plots (replicating Figure 3, 4, 5 from paper)\nprint(\"\\n[5/7] Creating scatter plots for feature categories...\")\n\n# Mean features scatter plot\nfig, axes = plt.subplots(5, 2, figsize=(15, 18))\naxes = axes.ravel()\n\nfor idx, col in enumerate(mean_features):\n    axes[idx].scatter(benign[col], benign.index, alpha=0.5, s=10, c='#2ecc71', label='Benign')\n    axes[idx].scatter(malignant[col], malignant.index, alpha=0.5, s=10, c='#e74c3c', label='Malignant')\n    axes[idx].set_xlabel(col, fontsize=10)\n    axes[idx].set_ylabel('Sample Index', fontsize=10)\n    axes[idx].set_title(f'Scatter: {col}', fontsize=11, fontweight='bold')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.suptitle('Scatter Plot of Mean Features (Replicating Paper Figure 3)', \n             fontsize=14, fontweight='bold', y=1.001)\nplt.tight_layout()\nplt.savefig('scatter_mean_features.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved as 'scatter_mean_features.png'\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Data Preparation**# ","metadata":{}},{"cell_type":"code","source":"\n# Remove 'id' column as it's not relevant for classification\nif 'id' in df.columns:\n    df = df.drop('id', axis=1)\n    print(\"\\n✓ 'id' column removed\")\n\n# Remove 'Unnamed: 32' column if it exists (common in this dataset)\nif 'Unnamed: 32' in df.columns:\n    df = df.drop('Unnamed: 32', axis=1)\n    print(\"✓ 'Unnamed: 32' column removed\")\n\nprint(f\"\\nDataset shape after cleaning: {df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 10. DATA NORMALIZATION\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATA NORMALIZATION\")\nprint(\"=\"*80)\n\n# Separate features and target\nX = df.drop('diagnosis', axis=1)\ny = df['diagnosis']\n\nprint(f\"\\nFeatures shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")\n\n# Standardize features using StandardScaler (as mentioned in the paper)\nscaler = StandardScaler()\nX_normalized = scaler.fit_transform(X)\n\n# Convert back to DataFrame for easier handling\nX_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)\n\nprint(\"\\n✓ Features normalized using StandardScaler (mean=0, std=1)\")\nprint(\"\\nNormalized data statistics:\")\nprint(X_normalized_df.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 11. TRAIN-TEST SPLIT\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAIN-TEST SPLIT\")\nprint(\"=\"*80)\n\n# Split data: 80% train, 20% test (paper used 70/30, but you specified 80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_normalized, y, test_size=0.20, random_state=42, stratify=y\n)\n\nprint(f\"\\nTraining set size: {X_train.shape[0]} samples ({(X_train.shape[0]/len(df))*100:.1f}%)\")\nprint(f\"Testing set size: {X_test.shape[0]} samples ({(X_test.shape[0]/len(df))*100:.1f}%)\")\n\nprint(f\"\\nTraining set class distribution:\")\nprint(f\"  Benign: {(y_train == 0).sum()} samples\")\nprint(f\"  Malignant: {(y_train == 1).sum()} samples\")\n\nprint(f\"\\nTesting set class distribution:\")\nprint(f\"  Benign: {(y_test == 0).sum()} samples\")\nprint(f\"  Malignant: {(y_test == 1).sum()} samples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 12. SAVE PROCESSED DATA\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SAVING PROCESSED DATA\")\nprint(\"=\"*80)\n\n# Save normalized data\nnp.save('X_train.npy', X_train)\nnp.save('X_test.npy', X_test)\nnp.save('y_train.npy', y_train)\nnp.save('y_test.npy', y_test)\n\nprint(\"\\n✓ Data saved successfully:\")\nprint(\"  - X_train.npy\")\nprint(\"  - X_test.npy\")\nprint(\"  - y_train.npy\")\nprint(\"  - y_test.npy\")\n\n# Save feature names\nfeature_names = X.columns.tolist()\nwith open('feature_names.txt', 'w') as f:\n    for feature in feature_names:\n        f.write(f\"{feature}\\n\")\nprint(\"  - feature_names.txt\")\n\n# Save scaler for future use\nimport pickle\nwith open('scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\nprint(\"  - scaler.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 13. SUMMARY REPORT\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY REPORT\")\nprint(\"=\"*80)\n\nsummary_report = f\"\"\"\nWISCONSIN DIAGNOSTIC BREAST CANCER DATASET - EDA REPORT\n{'='*80}\n\n1. DATASET OVERVIEW\n   - Total samples: {df.shape[0]}\n   - Total features: {df.shape[1] - 1} (after removing ID)\n   - Classes: Binary (Benign=0, Malignant=1)\n\n2. CLASS DISTRIBUTION\n   - Benign (B): {diagnosis_counts['B']} samples ({diagnosis_percentages['B']:.2f}%)\n   - Malignant (M): {diagnosis_counts['M']} samples ({diagnosis_percentages['M']:.2f}%)\n   - Balance: {'Relatively balanced' if abs(diagnosis_percentages['B'] - diagnosis_percentages['M']) < 20 else 'Imbalanced'}\n\n3. FEATURE CATEGORIES\n   - Mean features: {len(mean_features)}\n   - Standard Error features: {len(se_features)}\n   - Worst features: {len(worst_features)}\n   - Total: {len(mean_features) + len(se_features) + len(worst_features)} features\n\n4. DATA QUALITY\n   - Missing values: {'None' if len(missing_df) == 0 else f'{len(missing_df)} columns'}\n   - Duplicates: {df.duplicated().sum()}\n   - Data types: All numeric (after encoding)\n\n5. DATA PREPROCESSING\n   - Normalization: StandardScaler (mean=0, std=1)\n   - Train-Test Split: {(X_train.shape[0]/len(df))*100:.0f}% / {(X_test.shape[0]/len(df))*100:.0f}%\n   - Random State: 42 (for reproducibility)\n\n6. KEY FINDINGS\n   - Most correlated features with Malignant diagnosis:\n{chr(10).join([f'     • {feat}: {correlations_with_diagnosis[feat]:.3f}' for feat in correlations_with_diagnosis.head(5).index])}\n   \n   - Least correlated features with Malignant diagnosis:\n{chr(10).join([f'     • {feat}: {correlations_with_diagnosis[feat]:.3f}' for feat in correlations_with_diagnosis.tail(5).index])}\n\n7. LINEAR SEPARABILITY\n   - The dataset appears to be linearly separable based on visualization\n   - This aligns with the paper's findings that linear classifiers performed well\n   - Mean features show clear separation between benign and malignant cases\n\n8. FILES GENERATED\n   - Training data: X_train.npy, y_train.npy\n   - Testing data: X_test.npy, y_test.npy\n   - Scaler: scaler.pkl\n   - Feature names: feature_names.txt\n   - Visualizations: \n     • class_distribution.png\n     • mean_features_distribution.png\n     • mean_features_boxplot.png\n     • correlation_matrix_full.png\n     • correlation_with_diagnosis.png\n     • scatter_mean_features.png\n\n9. NEXT STEPS (FOR OTHER TEAM MEMBERS)\n   - Person 2-6: Load the processed data using:\n     ```python\n     X_train = np.load('X_train.npy')\n     X_test = np.load('X_test.npy')\n     y_train = np.load('y_train.npy')\n     y_test = np.load('y_test.npy')\n     ```\n   - Implement ML algorithms: Linear Regression, MLP, Nearest Neighbor, \n     Softmax Regression, SVM, GRU-SVM\n   - Target: >90% test accuracy (paper achieved ~99% with MLP)\n\n{'='*80}\nReport generated successfully!\n\"\"\"\n\nprint(summary_report)\n\n# Save report to file\nwith open('EDA_REPORT.txt', 'w', encoding='utf-8') as f:\n    f.write(summary_report)\n\nprint(\"\\n✓ Complete EDA report saved as 'EDA_REPORT.txt'\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATA PREPARATION & EDA COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*80)\nprint(\"\\nYou can now share the processed data files with your team members.\")\nprint(\"All visualizations and the detailed report are ready for your project submission.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nmodel = Sequential([\n    Dense(32, input_shape=(X_train.shape[1],), activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')\n],name='ANN_Model')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer='Adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=50,\n    batch_size=16,\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"model.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy*100:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss over epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy over epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ny_pred_prob = model.predict(X_test) \ny_pred = (y_pred_prob > 0.5).astype(int) \n\ncm = confusion_matrix(y_test, y_pred)\n\nclass_names = ['Benign (non-cancerous)', 'Malignant (cancerous)']  \nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix for ANN Model')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}